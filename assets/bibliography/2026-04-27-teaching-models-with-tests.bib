@misc{huggingface_rlhf_blog,
  title={Illustrating Reinforcement Learning from Human Feedback},
  author={von Werra, Leandro and contributors},
  year={2022},
  howpublished={Hugging Face Blog},
  url={https://huggingface.co/blog/rlhf}
}

@misc{tulu3_open_instruct,
  title={Tulu 3: Open Instruction-tuned Models},
  author={{Allen Institute for AI}},
  year={2025},
  howpublished={GitHub repository},
  url={https://github.com/allenai/open-instruct}
}

@article{grpo2025,
  title={Group Relative Policy Optimization},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2503.06639},
  url={https://arxiv.org/abs/2503.06639}
}

@article{rlvr_world2025,
  title={RLVR-World: Reinforcement Learning with Verified Rewards for World Models},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2505.13934},
  url={https://arxiv.org/abs/2505.13934}
}

@article{visual_rft2025,
  title={Visual-RFT: Reinforcement Learning with Verified Rewards for Vision-Language Models},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2503.01785},
  url={https://arxiv.org/abs/2503.01785}
}

@article{rlpr2025,
  title={RLPR: Reinforcement Learning from Probabilistic Rewards},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2506.18254},
  url={https://arxiv.org/abs/2506.18254}
}

@article{hero2025,
  title={HERO: Hybrid Ensemble Reward Optimization},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2510.07242},
  url={https://arxiv.org/abs/2510.07242}
}

@article{specrl2025,
  title={SPEC-RL: Speculative Rollouts for Reinforcement Learning with Verified Rewards},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2509.23232},
  url={https://arxiv.org/abs/2509.23232}
}

@article{cure2025,
  title={CURE: Curriculum Reinforcement Learning for Exploration},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2508.11016},
  url={https://arxiv.org/abs/2508.11016}
}

@article{latr2025,
  title={LATR: Learning to Align Trajectories with Rollouts},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2510.24302},
  url={https://arxiv.org/abs/2510.24302}
}

@article{composite_rewards2025,
  title={Reward Hacking Mitigation using Verifiable Composite Rewards},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2509.15557},
  url={https://arxiv.org/abs/2509.15557}
}

@article{noisy_verifiers2025,
  title={Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2510.00915},
  url={https://arxiv.org/abs/2510.00915}
}

@article{verifybench2025,
  title={VerifyBench: Benchmarking Verifiers for Reasoning Models},
  author={Anonymous},
  year={2025},
  archivePrefix={arXiv},
  eprint={2507.09884},
  url={https://arxiv.org/abs/2507.09884}
}

@misc{weng_reward_hacking,
  title={Reward Hacking in Reinforcement Learning},
  author={Weng, Lilian},
  year={2024},
  howpublished={Blog post},
  url={https://lilianweng.github.io/posts/2024-11-28-reward-hacking/}
}

@misc{promptfoo_rlvr,
  title={RLVR Explained: Why Verified Rewards Matter},
  author={{promptfoo contributors}},
  year={2025},
  howpublished={Blog post},
  url={https://www.promptfoo.dev/blog/rlvr-explained/}
}
